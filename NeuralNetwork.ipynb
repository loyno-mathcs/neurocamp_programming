{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniature Neural Network Example\n",
    "\n",
    "This notebook implements a neural network that, when sufficiently trained, can execute exclusive or (XOR) decisions.\n",
    "\n",
    "This is based on [the implementation by Konstantinos Kitsios](https://towardsdatascience.com/how-to-build-a-simple-neural-network-from-scratch-with-python-9f011896d2f3), with [source code on GitLab](https://gitlab.com/kitsiosk/xor-neural-net/tree/master).\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "First, we'll be working with numerical matrices. Since these data structures are not part of the base collection of Python 3 libraries, we'll need to import a new library `numpy` to take care of that. When we import, we're providing an alias `np` so that we can use this to refer to that library in an abbreviated manner in future code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this library a bit.\n",
    "\n",
    "## Intro to matrices in NumPy\n",
    "\n",
    "Matrices are mathematical structures represented as a grid of numbers, for example:\n",
    "\n",
    "$$\\begin{bmatrix} \n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In NumPy, matrices are represented as arrays filled with arrays of numbers, each representing a row of the matrix.\n",
    "\n",
    "We can create matrices filled with random numbers, or with ones or zeros easily using NumPy's convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43773836,  0.30405338],\n",
       "       [-0.12310372,  1.62265474],\n",
       "       [ 1.07894756,  0.36820657]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69946656, -1.02740829, -0.24420973, -0.2411146 , -0.61887109],\n",
       "       [-0.4944247 ,  0.56966557,  1.07035792, -1.14609221,  0.69111952],\n",
       "       [ 0.82643473,  0.92277872, -0.25948179,  0.72127418,  1.56117944],\n",
       "       [-0.25629474,  0.65354462, -0.30536742, -0.96844837, -0.15398082],\n",
       "       [-0.56258033,  0.07955386, -1.59653425, -0.22107714,  1.68083691]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((4, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also multiply matrices, either as a dot-product or a cross-product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(3, 3)\n",
    "B = np.random.randn(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14204565, -0.70993346, -0.84807117],\n",
       "       [ 0.0107026 , -0.46219455,  1.48010672],\n",
       "       [ 0.45563562,  1.22179254,  0.03044326]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18889061, -0.25707058],\n",
       "       [ 1.24349507, -0.34735751],\n",
       "       [ 0.98607691, -0.4372636 ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21801415, -0.16019268,  0.09758401],\n",
       "       [ 0.51412619,  1.84050541,  0.57101901],\n",
       "       [ 0.01331173,  0.0300194 , -1.40401428]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cross(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.69223107,  0.58091562],\n",
       "       [ 0.88678405, -0.48940137],\n",
       "       [ 1.63537768, -0.55484106]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other convenient functions for creating and manipulating matrices as well, which you can read about in the NumPy documentation: https://docs.scipy.org/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Neural Network\n",
    "\n",
    "### The Goal\n",
    "\n",
    "To start, let's define the problem we want our neural network to solve.\n",
    "\n",
    "Given two inputs $a$ and $b$, we would like our network to decide the result $c$ as follows:\n",
    "\n",
    "$$\\frac{\\left.\\begin{matrix}\n",
    "a & b &\n",
    "\\end{matrix}\\right|\\begin{matrix}\n",
    "& \\hat{y}\n",
    "\\end{matrix}}{\\left.\\begin{matrix}\n",
    "0 & 0 & \\\\\n",
    "0 & 1 & \\\\\n",
    "1 & 0 & \\\\\n",
    "1 & 1 & \n",
    "\\end{matrix}\\right|\\begin{matrix}\n",
    "& 0 \\\\ & 1  \\\\ & 1 \\\\ & 0\n",
    "\\end{matrix}}$$\n",
    "\n",
    "Neural networks work by passing inputs through successive layers of artificial neurons. The inputs are analogous to neurotransmitter activations of biological neurons, and the neurons apply a transformation function to the inputs when activated.\n",
    "\n",
    "In practice, neural networks can have many millions of neurons in many hundreds or thousands of layers, with most of those layers hidden from inspection.\n",
    "\n",
    "For this example, we'll use a simple 3-layer neural network: an input layer ($X$ in the diagram below) with only our two inputs $a$ and $b$, a single hidden layer ($A1$ in the diagram), and the output layer ($A2$ in the diagram) with our output $\\hat{y}$.\n",
    "\n",
    "![Neural network diagram](https://cdn-images-1.medium.com/max/1600/1*dFbZFuwqoR2YIHS4YcC9Jg.jpeg)\n",
    "\n",
    "You'll also notice in the diagram that we've defined transformation information along the edges moving from one layer to the next. These choices are important, as they define the behavior of neural firing in the network.\n",
    "\n",
    "For our activation functions (simulating the activation of a neuron by neurotransmitter stimulus), we're using the hyperbolic tangent (which we'll call $h$) and sigmoid (which we'll call $g$) functions. The sigmoid function is a good choice for the last layer in this network because it outputs values between 0 and 1, with a relatively sharp transition from one to the other. The hyperbolic tangent function is a reasonable choice for the hidden layer, but many other functions could work here. So, our activation functions are:\n",
    "\n",
    "$$\\begin{align}\n",
    "h(z) &= \\tanh(x) \\\\\n",
    "g(z) &= \\frac{1}{1 + e^{-z}}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "When we calculate the \"activated\" value sent to the next layer, these are computed as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "A1 &= h(W1\\cdot X + b1) \\\\\n",
    "A2 &= g(W2\\cdot A1 + b2)\n",
    "\\end{align}$$\n",
    "\n",
    "The values $W1$ and $W2$ are called \"weights\", and the values $b1$ and $b2$ are called \"biases\". Together, these weights and biases make up the parameters to be learned by the neural network. These values are generally matrices.\n",
    "\n",
    "To start, let's implement the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     z |               g(z)\n",
      "   -10 | 0.0000453978687024\n",
      "    -9 | 0.0001233945759862\n",
      "    -8 | 0.0003353501304665\n",
      "    -7 | 0.0009110511944006\n",
      "    -6 | 0.0024726231566348\n",
      "    -5 | 0.0066928509242849\n",
      "    -4 | 0.0179862099620916\n",
      "    -3 | 0.0474258731775668\n",
      "    -2 | 0.1192029220221175\n",
      "    -1 | 0.2689414213699951\n",
      "     0 | 0.5000000000000000\n",
      "     1 | 0.7310585786300049\n",
      "     2 | 0.8807970779778823\n",
      "     3 | 0.9525741268224334\n",
      "     4 | 0.9820137900379085\n",
      "     5 | 0.9933071490757153\n",
      "     6 | 0.9975273768433653\n",
      "     7 | 0.9990889488055994\n",
      "     8 | 0.9996646498695336\n",
      "     9 | 0.9998766054240137\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# This computes on matrices, but let's check some single values to get a feel for it.\n",
    "print(f\"{'z':>6s} | {'g(z)':>18s}\")\n",
    "for z in range(-10, 10):\n",
    "    print(f\"{z:>6d} | {sigmoid(z):>18.16f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll initialize our parameter values based on the number of neurons in each layer of the neural network. Our weights need to be initialized with random values from a normal distribution, and our biases start at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.89857283,  0.79062035],\n",
      "       [-1.26652309,  0.57572595]]), 'b1': array([[0.],\n",
      "       [0.]]), 'W2': array([[-0.41978651, -0.03815815]]), 'b2': array([[0.]])}\n",
      "{'W1': array([[ 0.30117773, -0.83138512,  0.56300765, -0.15833648],\n",
      "       [-1.01901354, -1.63416825, -0.23911938,  1.35304578],\n",
      "       [ 0.53346752,  0.25131605, -0.8339447 , -0.49249342]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.31271814, -0.29261628,  0.67403643],\n",
      "       [ 0.50532101,  2.58823728, -1.606086  ]]), 'b2': array([[0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "def init_parameters(num_inputs, num_hidden, num_outputs):\n",
    "    # Weights (np.random.randn returns random normally distributed matrices)\n",
    "    W1 = np.random.randn(num_hidden, num_inputs)\n",
    "    W2 = np.random.randn(num_outputs, num_hidden)\n",
    "    # Biases (np.zeros returns matrices with the specified dimensions with all elements zero)\n",
    "    b1 = np.zeros((num_hidden, 1))\n",
    "    b2 = np.zeros((num_outputs, 1))\n",
    "    # Create and return our params dictionary\n",
    "    params = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    return params\n",
    "\n",
    "print(init_parameters(2, 2, 1))\n",
    "print(init_parameters(4, 3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to create the function that will propagate activations forward through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "[[0.5        0.38686082 0.50245512 0.38807735]]\n"
     ]
    }
   ],
   "source": [
    "def forward_prop(inputs, params):\n",
    "    # Take apart arguments and put them in math notation\n",
    "    X = inputs\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    # Propagate the inputs into the hidden layer\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    # Propagate the hidden layer into the output layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    # Remember the values of A1 and A2 for next time\n",
    "    cache = {\n",
    "        'A1': A1,\n",
    "        'A2': A2\n",
    "    }\n",
    "    # Return a tuple of the result and the cache\n",
    "    return A2, cache\n",
    "\n",
    "# Let's try this out...\n",
    "p = init_parameters(2,2,1) # Set our parameters\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # The 4 training examples by columns\n",
    "print(X)\n",
    "Y_hat, cache = forward_prop(X, p)\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've now calculated a result of the network, we can also compute the loss function. This function tells us how far from the correct the network's prediction was. For this, we're using a calculation method called the \"Cross Entropy Loss Function\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(prediction, truth, num_training_examples):\n",
    "    A2 = prediction\n",
    "    Y = truth\n",
    "    m = num_training_examples\n",
    "    cost = -np.sum(np.multiply(Y, np.log(A2)) + np.multiply(1-Y, np.log(1-A2)))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Let's get a sense for this function:\n",
    "Y = np.array([[0, 1, 1, 0]]) # ground truth\n",
    "m = X.shape[1] # number of training examples\n",
    "# Start with something really dumb\n",
    "Y_hat = np.array([[0.5, 0.5, 0.5, 0.5]])\n",
    "print(calculate_cost(Y_hat, Y, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the learning! Neural networks also propagate information backward in order to learn better performance. In our case, we'll be using a method called \"Gradient Descent\" to train our neural network. The input for that method will be the gradients of the loss function we just implemented at the values of our parameters (weights and biases).\n",
    "\n",
    "A gradient of a function at a specific point is the slope of a line in a particular direction on the surface defined by that function. You can think of the gradient in the x direction of a function as a measure of the steepness of the surface when looking in the x-axis direction.\n",
    "\n",
    "We'll be taking our gradients in the directions of the weights $W1$ and $W2$.\n",
    "\n",
    "Note: This is the toughest part of a neural network algorithm, so if this piece doesn't make sense to you, don't worry too much about it. This will become much clearer after a bit of multivariable calculus.\n",
    "\n",
    "Let's go ahead and implement a function that performs the backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW1': array([[ 0.0511386 ,  0.05107287],\n",
      "       [-0.00272696, -0.00308779]]), 'db1': array([[ 0.05038723],\n",
      "       [-0.00230877]]), 'dW2': array([[-0.09889518, -0.20508841]]), 'db2': array([[-0.05565168]])}\n"
     ]
    }
   ],
   "source": [
    "def back_prop(inputs, truth, cache, params, num_training_examples):\n",
    "    # Handle arguments\n",
    "    X = inputs\n",
    "    Y = truth\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    W2 = params['W2']\n",
    "    m = num_training_examples\n",
    "    # Compute gradients\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))\n",
    "    dW1 = np.dot(dZ1, X.T)/m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n",
    "    # Return gradients\n",
    "    return {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1,\n",
    "        'dW2': dW2,\n",
    "        'db2': db2\n",
    "    }\n",
    "\n",
    "# Let's test this out.\n",
    "gradients = back_prop(X, Y, cache, p, m)\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our gradient calculations done, we can move on to implementing the rest of the Gradient Descent method: the updates to our parameters. This is the piece we refer to when we say that a neural network \"learns\".\n",
    "\n",
    "To implement this, we need a new \"hyperparameter\" -- this defines the learning/training mechanism. This parameter is the learning rate, and it controls how much the Gradient Descent step can change the parameters it is learning at each iteration of the algorithm.\n",
    "\n",
    "Let's implement a function to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[0.00580873, 0.46564667],\n",
      "       [0.39048838, 0.54514684]]), 'b1': array([[0.],\n",
      "       [0.]]), 'W2': array([[-1.10948328,  0.04374929]]), 'b2': array([[0.]])} {'W1': array([[0.00580873, 0.46564667],\n",
      "       [0.39048838, 0.54514684]]), 'b1': array([[0.],\n",
      "       [0.]]), 'W2': array([[-1.10948328,  0.04374929]]), 'b2': array([[0.]])}\n",
      "{'W1': array([[0.00580873, 0.46564667],\n",
      "       [0.39048838, 0.54514684]]), 'b1': array([[0.],\n",
      "       [0.]]), 'W2': array([[-1.10948328,  0.04374929]]), 'b2': array([[0.]])} {'W1': array([[0.00069487, 0.46053938],\n",
      "       [0.39076108, 0.54545562]]), 'b1': array([[-0.00503872],\n",
      "       [ 0.00023088]]), 'W2': array([[-1.09959377,  0.06425813]]), 'b2': array([[0.00556517]])}\n",
      "{'W1': array([[0.00580873, 0.46564667],\n",
      "       [0.39048838, 0.54514684]]), 'b1': array([[0.],\n",
      "       [0.]]), 'W2': array([[-1.10948328,  0.04374929]]), 'b2': array([[0.]])} {'W1': array([[-0.00441899,  0.45543209],\n",
      "       [ 0.39103377,  0.5457644 ]]), 'b1': array([[-0.01007745],\n",
      "       [ 0.00046175]]), 'W2': array([[-1.08970425,  0.08476697]]), 'b2': array([[0.01113034]])}\n",
      "{'W1': array([[0.00580873, 0.46564667],\n",
      "       [0.39048838, 0.54514684]]), 'b1': array([[0.],\n",
      "       [0.]]), 'W2': array([[-1.10948328,  0.04374929]]), 'b2': array([[0.]])} {'W1': array([[-0.00953285,  0.45032481],\n",
      "       [ 0.39130647,  0.54607318]]), 'b1': array([[-0.01511617],\n",
      "       [ 0.00069263]]), 'W2': array([[-1.07981473,  0.10527582]]), 'b2': array([[0.0166955]])}\n",
      "{'W1': array([[0.00580873, 0.46564667],\n",
      "       [0.39048838, 0.54514684]]), 'b1': array([[0.],\n",
      "       [0.]]), 'W2': array([[-1.10948328,  0.04374929]]), 'b2': array([[0.]])} {'W1': array([[-0.01464671,  0.44521752],\n",
      "       [ 0.39157916,  0.54638196]]), 'b1': array([[-0.02015489],\n",
      "       [ 0.00092351]]), 'W2': array([[-1.06992521,  0.12578466]]), 'b2': array([[0.02226067]])}\n"
     ]
    }
   ],
   "source": [
    "def update_params(params, gradients, learning_rate):\n",
    "    W1 = params['W1'] - learning_rate * gradients['dW1']\n",
    "    b1 = params['b1'] - learning_rate * gradients['db1']\n",
    "    W2 = params['W2'] - learning_rate * gradients['dW2']\n",
    "    b2 = params['b2'] - learning_rate * gradients['db2']\n",
    "    new_params = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    return new_params\n",
    "\n",
    "# Let's try it out\n",
    "for learning_rate in [0.1 * x for x in range(0, 5)]:\n",
    "    new_params = update_params(p, gradients, learning_rate)\n",
    "    print(p, new_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put all this together into a new function that iterates a certain number of times, or until our cost is low enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(inputs, truth, num_inputs, num_hidden, num_outputs, num_iterations, learning_rate, num_training_examples):\n",
    "    X = inputs\n",
    "    Y = truth\n",
    "    params = init_parameters(num_inputs, num_hidden, num_outputs)\n",
    "    for k in range(0, num_iterations + 1):\n",
    "        Y_hat, cache = forward_prop(X, params)\n",
    "        cost = calculate_cost(Y_hat, Y, num_training_examples)\n",
    "        gradients = back_prop(X, Y, cache, params, num_training_examples)\n",
    "        params = update_params(params, gradients, learning_rate)\n",
    "        if (k % 100 == 0):\n",
    "            print(f\"Cost after iteration # {k:d}: {cost:f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to train something in a minute -- first, let's get the other side of this coin finished: the ability to predict from the neural network. We'll predict by setting a threshold of 0.5, applying the trained parameters to our inputs, and returning the guessed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, params):\n",
    "    Y_hat, cache = forward_prop(inputs, params)\n",
    "    Y_hat = np.squeeze(Y_hat)\n",
    "    if (Y_hat >= 0.5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these functions implemented, we're finally ready to train a neural network and get a prediction! Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration # 0: 0.856267\n",
      "Cost after iteration # 100: 0.282135\n",
      "Cost after iteration # 200: 0.072281\n",
      "Cost after iteration # 300: 0.031606\n",
      "Cost after iteration # 400: 0.019622\n",
      "Cost after iteration # 500: 0.014160\n",
      "Cost after iteration # 600: 0.011064\n",
      "Cost after iteration # 700: 0.009075\n",
      "Cost after iteration # 800: 0.007692\n",
      "Cost after iteration # 900: 0.006675\n",
      "Cost after iteration # 1000: 0.005895\n",
      "Neural network prediction for inputs (0, 0) is 0\n",
      "Neural network prediction for inputs (0, 1) is 1\n",
      "Neural network prediction for inputs (1, 0) is 1\n",
      "Neural network prediction for inputs (1, 1) is 0\n"
     ]
    }
   ],
   "source": [
    "# Reset the random number seed\n",
    "np.random.seed(2)\n",
    "\n",
    "# Setup our 4 training examples by column\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "\n",
    "# Our ground truth: the outputs of the XOR for every example in X\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "# The number of training examples\n",
    "num_training_examples = X.shape[1]\n",
    "\n",
    "# The hyper-parameters defining the neural model and learning characteristics\n",
    "num_input_neurons = 2\n",
    "num_hidden_neurons = 2\n",
    "num_output_neurons = 1\n",
    "num_iterations = 1000\n",
    "learning_rate = 0.3\n",
    "\n",
    "# Train the neural network\n",
    "trained_params = train_model(X, Y, num_input_neurons, num_hidden_neurons, num_output_neurons,\n",
    "                             num_iterations, learning_rate, num_training_examples)\n",
    "\n",
    "# Get a prediction! We can try any of these: (0, 0), (0, 1), (1, 0). (1, 1)\n",
    "for x1, x2 in [(a, b) for a in range(2) for b in range(2)]:\n",
    "    X_test = np.array([[x1], [x2]])\n",
    "    Y_predict = predict(X_test, trained_params)\n",
    "    print(f\"Neural network prediction for inputs ({X_test[0][0]}, {X_test[1][0]}) is {Y_predict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We've built a neural network from scratch, albeit a simple one!\n",
    "\n",
    "Here are some ways to further explore this:\n",
    "\n",
    "Try modifying the hyperparameters.\n",
    "\n",
    "* What happens if you change the number of neurons in a layer?\n",
    "* What happens if you modify the learning rate?\n",
    "* What happens if you change the number of iterations?\n",
    "* Can you make a prediction fail just by changing hyperparameters?\n",
    "\n",
    "Generalize this model.\n",
    "\n",
    "* This model currently executes an XOR operation on two inputs. How would you extend this to a mutual XOR on multiple inputs?\n",
    "\n",
    "Can you think of other ways to modify what's done here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
